---
import BaseLayout from '../layouts/BaseLayout.astro';
---

<BaseLayout title="Projects" description="Research projects and tools by Prabhu Vellaisamy.">
  <header class="page-header">
    <h1>Projects</h1>
    <p class="subtitle">Research projects, tools, and hardware designs</p>
  </header>

  <section class="section">
    <h2 class="section-title">Research Projects</h2>

    <article class="project">
      <h3 class="project-title">SKIP</h3>
      <p class="project-description">
        A novel PyTorch-based profiling tool for analyzing operator-kernel dynamics in large language model (LLM)
        inference workloads. SKIP revealed that NVIDIA GH200 incurs 2.8x more prefill latency compared to traditional
        PCIe-connected Intel x86 CPU - H100 GPU systems, and exhibits a 4x larger CPU-bounded region due to Grace CPU
        inefficiencies. This work was conducted during an internship at Samsung Semiconductor and led to a joint
        CMU-Samsung paper accepted at ISPASS 2025.
      </p>
      <div class="project-tags">
        <span class="tag">Python</span>
        <span class="tag">PyTorch</span>
        <span class="tag">LLM Inference</span>
        <span class="tag">GPU Profiling</span>
      </div>
    </article>

    <article class="project">
      <h3 class="project-title">Tempus Core</h3>
      <p class="project-description">
        An INT8 temporal-unary convolution core designed for NVDLA (NVIDIA Deep Learning Accelerator).
        Tempus Core achieves 53% area reduction, 44% power reduction, and 5x iso-area throughput improvement
        compared to baseline designs. Published at DATE 2025.
      </p>
      <div class="project-tags">
        <span class="tag">SystemVerilog</span>
        <span class="tag">Unary Computing</span>
        <span class="tag">DLA</span>
        <span class="tag">Edge AI</span>
      </div>
    </article>

    <article class="project">
      <h3 class="project-title">TNNGen</h3>
      <p class="project-description">
        An automated design framework for translating Temporal Neural Networks (TNNs) from PyTorch models
        to post-layout netlists. TNNGen supports automated design generation for 7 different modalities
        and significantly accelerates the design process for neuromorphic sensory processing units.
        Published at ISCAS 2024 and selected for publication in TCAS-II 2024.
      </p>
      <div class="project-tags">
        <span class="tag">Python</span>
        <span class="tag">PyTorch</span>
        <span class="tag">Cadence Tools</span>
        <span class="tag">Neuromorphic</span>
      </div>
    </article>

    <article class="project">
      <h3 class="project-title">TNN7</h3>
      <p class="project-description">
        A custom predictive 7nm open-source PDK for Temporal Neural Networks, developed as an extension to ASAP7.
        TNN7 consists of nine custom hard macros optimized for TNN designs, achieving 14% power reduction,
        16% delay reduction, 28% area reduction, and 45% energy-delay product (EDP) reduction over baseline designs.
        Published at ISVLSI 2022.
      </p>
      <div class="project-tags">
        <span class="tag">ASAP7 PDK</span>
        <span class="tag">Custom Macros</span>
        <span class="tag">7nm</span>
        <span class="tag">Open Source</span>
      </div>
    </article>

    <article class="project">
      <h3 class="project-title">tubGEMM</h3>
      <p class="project-description">
        An ultra low-power hybrid temporal-unary-binary general matrix multiplication (GEMM) unit designed
        for edge AI inference. tubGEMM exploits sparsity while maintaining energy efficiency, making it
        suitable for resource-constrained deep learning accelerators. Developed using TSMC N5 process technology
        at MediaTek. Published at ISVLSI 2023.
      </p>
      <div class="project-tags">
        <span class="tag">SystemVerilog</span>
        <span class="tag">TSMC N5</span>
        <span class="tag">GEMM</span>
        <span class="tag">Unary Computing</span>
      </div>
    </article>

    <article class="project">
      <h3 class="project-title">OzMAC</h3>
      <p class="project-description">
        A novel energy-efficient bit-serial multiply-accumulate (MAC) unit that exploits sparsity for
        deep learning inference. Designed using TSMC N5 process technology during an internship at MediaTek.
        Published at VLSI-SoC 2024.
      </p>
      <div class="project-tags">
        <span class="tag">SystemVerilog</span>
        <span class="tag">TSMC N5</span>
        <span class="tag">MAC Unit</span>
        <span class="tag">Sparsity</span>
      </div>
    </article>

    <article class="project">
      <h3 class="project-title">Mugi</h3>
      <p class="project-description">
        Value Level Parallelism architecture for efficient large language model inference.
        Accepted at ASPLOS 2026.
      </p>
      <div class="project-tags">
        <span class="tag">LLM</span>
        <span class="tag">Architecture</span>
        <span class="tag">Parallelism</span>
      </div>
    </article>

    <article class="project">
      <h3 class="project-title">Catwalk</h3>
      <p class="project-description">
        Unary Top-K implementation for efficient Ramp-No-Leak neuron design in Temporal Neural Networks.
        This work received the Amar Mukherjee Best Paper Award at ISVLSI 2025.
      </p>
      <div class="project-tags">
        <span class="tag">Unary Computing</span>
        <span class="tag">TNN</span>
        <span class="tag">Neuron Design</span>
      </div>
    </article>
  </section>

  <section class="section">
    <h2 class="section-title">Teaching</h2>
    <p>Teaching Assistant at Carnegie Mellon University:</p>
    <ul>
      <li>
        <strong>18-340/640: Hardware Arithmetic for Machine Learning</strong><br>
        <span class="course-semesters">Fall 2025, Fall 2024, Fall 2023, Fall 2021</span>
      </li>
      <li>
        <strong>18-743: Neuromorphic Computer Architecture and Processor Design</strong><br>
        <span class="course-semesters">Spring 2025, Spring 2024, Spring 2023, Spring 2022, Spring 2021</span>
      </li>
      <li>
        <strong>18-740: Modern Computer Architecture</strong><br>
        <span class="course-semesters">Fall 2022</span>
      </li>
    </ul>
  </section>

  <section class="section">
    <h2 class="section-title">Mentorship</h2>
    <p>
      Directed a team of 9 undergraduate and graduate research assistants in designing unary-based
      deep learning architectures, contributing to the development of design methodologies and
      automation flows at CMU's Computer Architecture Lab.
    </p>
  </section>
</BaseLayout>

<style>
  .project {
    margin-bottom: var(--spacing-xl);
    padding-bottom: var(--spacing-lg);
    border-bottom: 1px solid var(--color-border);
  }

  .project:last-child {
    border-bottom: none;
  }

  .project-title {
    font-size: 1.2rem;
    margin: 0 0 var(--spacing-sm) 0;
    color: var(--color-heading);
  }

  .project-description {
    margin-bottom: var(--spacing-md);
  }

  .project-tags {
    display: flex;
    gap: var(--spacing-sm);
    flex-wrap: wrap;
  }

  .course-semesters {
    font-size: 0.9rem;
    color: var(--color-text-light);
  }
</style>
